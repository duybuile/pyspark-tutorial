{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to get data from the **IBM Watson Studio Community**, create a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to the Watson Machine Learning repository, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3 and Apache Spark 2.3.\n",
    "\n",
    "You will use a publicly available data set, GoSales Transactions for Naive Bayes Model, which details anonymous outdoor equipment purchases. Use the details of this data set to predict clients' interests in terms of product line, such as golf accessories, camping equipment and so on.\n",
    "\n",
    "## Learning goals\n",
    "You will learn how to:\n",
    "\n",
    "- Load a CSV file into an Apache Spark DataFrame.\n",
    "- Explore data.\n",
    "- Prepare data for training and evaluation.\n",
    "- Create an Apache Spark machine learning pipeline.\n",
    "- Train and evaluate a model.\n",
    "- Store a pipeline and model in Watson Machine Learning (WML) repository.\n",
    "- Deploy a model for online scoring using the Watson Machine Learning (WML) API.\n",
    "- Score sample scoring data using the WML API.\n",
    "- Explore and visualize the prediction result using the plotly package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up\n",
    "\n",
    "Install **pySpark** by using Anaconda `conda install -c conda-forge pyspark`\n",
    "\n",
    "Install **wget** `conda install -c menpo wget`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "\n",
    "In this section you will load the data as an **Apache Spark DataFrame** and perform a basic exploration.\n",
    "\n",
    "Load the data to the Spark DataFrame by using *wget* to upload the data to gpfs and then use spark read method to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 / unknownGoSales_Tx_NaiveBayes.csv\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600'\n",
    "filename = wget.download(link_to_data)\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_data = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRODUCT_LINE: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MARITAL_STATUS: string (nullable = true)\n",
      " |-- PROFESSION: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---+--------------+------------+\n",
      "|        PRODUCT_LINE|GENDER|AGE|MARITAL_STATUS|  PROFESSION|\n",
      "+--------------------+------+---+--------------+------------+\n",
      "|Personal Accessories|     M| 27|        Single|Professional|\n",
      "|Personal Accessories|     F| 39|       Married|       Other|\n",
      "|Mountaineering Eq...|     F| 39|       Married|       Other|\n",
      "|Personal Accessories|     F| 56|   Unspecified| Hospitality|\n",
      "|      Golf Equipment|     M| 45|       Married|     Retired|\n",
      "|      Golf Equipment|     M| 45|       Married|     Retired|\n",
      "|   Camping Equipment|     F| 39|       Married|       Other|\n",
      "|   Camping Equipment|     F| 49|       Married|       Other|\n",
      "|  Outdoor Protection|     F| 49|       Married|       Other|\n",
      "|      Golf Equipment|     M| 47|       Married|     Retired|\n",
      "|      Golf Equipment|     M| 47|       Married|     Retired|\n",
      "|Mountaineering Eq...|     M| 21|        Single|      Retail|\n",
      "|Personal Accessories|     F| 66|       Married|       Other|\n",
      "|   Camping Equipment|     F| 35|       Married|Professional|\n",
      "|Mountaineering Eq...|     M| 20|        Single|       Sales|\n",
      "|Mountaineering Eq...|     M| 20|        Single|       Sales|\n",
      "|Mountaineering Eq...|     M| 20|        Single|       Sales|\n",
      "|Personal Accessories|     F| 37|        Single|       Other|\n",
      "|   Camping Equipment|     M| 42|       Married|       Other|\n",
      "|   Camping Equipment|     F| 24|       Married|      Retail|\n",
      "+--------------------+------+---+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 60252\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \" + str(df_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an Apache Spark machine learning model\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* 3.1 Prepare data\n",
    "* 3.2 Create an Apache Spark machine learning pipeline\n",
    "* 3.3 Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 48176\n",
      "Number of testing records : 10860\n",
      "Number of prediction records : 1216\n"
     ]
    }
   ],
   "source": [
    "splitted_data = df_data.randomSplit([0.8, 0.18, 0.02], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))\n",
    "print(\"Number of prediction records : \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see your data has been successfully split into three data sets:\n",
    "\n",
    "* The train data set, which is the largest group, is used for training.\n",
    "* The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "* The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create the pipeline\n",
    "In this section you will create an Apache Spark machine learning pipeline and then train the model.\n",
    "\n",
    "In the first step you need to import the Apache Spark machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
