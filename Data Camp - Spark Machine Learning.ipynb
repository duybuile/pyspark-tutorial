{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Spark Program on Jupyter\n",
    "\n",
    "Import the SparkSession module from pyspark.sql and build a **SparkSession** with the __`builder()`__ method. Afterwards, you can set the master URL to connect to, the application name, add some additional configuration like the executor memory and then lastly, use __`getOrCreate()`__ to either get the current Spark session or to create one if there is none running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
    "\n",
    "* __Longitude__ refers to the angular distance of a geographic place north or south of the earth’s equator for each block group;\n",
    "* __Latitude__ refers to the angular distance of a geographic place east or west of the earth’s equator for each block group;\n",
    "* __Housing median age__ is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values;\n",
    "* __Total rooms__ is the total number of rooms in the houses per block group;\n",
    "* __Total bedrooms__ is the total number of bedrooms in the houses per block group;\n",
    "* __Population__ is the number of inhabitants of a block group;\n",
    "* __Households__ refers to units of houses and their occupants per block group;\n",
    "* __Median income__ is used to register the median income of people that belong to a block group; And,\n",
    "* __Median house value__ is the dependent variable and refers to the median house value per block group.\n",
    "\n",
    "The __Median house value__ is the dependent variable and will be assigned the role of the target variable in your ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you’ll use the __`textFile()`__ method to read in the data from the folder that you downloaded it to RDDs. This method takes an URI for the file, which is in this case the local path of your machine, and reads it as a collection of lines. For all convenience, you’ll not only read in the .data file, but also the .domain file that contains the header. This will allow you to double check the order of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "rdd = sc.textFile('data/CaliforniaHousing/cal_housing.data')\n",
    "\n",
    "# Load in the header\n",
    "header = sc.textFile('data/CaliforniaHousing/cal_housing.domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Important to understand here is that, because Spark’s execution is __“lazy”__ execution, nothing has been executed yet. Your data hasn’t been actually read in. The rdd and header variables are actually just concepts in your mind. You have to push Spark to work for you, so let’s use the __`collect()`__ method to look at the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude: continuous.',\n",
       " 'latitude: continuous.',\n",
       " 'housingMedianAge: continuous. ',\n",
       " 'totalRooms: continuous. ',\n",
       " 'totalBedrooms: continuous. ',\n",
       " 'population: continuous. ',\n",
       " 'households: continuous. ',\n",
       " 'medianIncome: continuous. ',\n",
       " 'medianHouseValue: continuous. ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collect()` method brings the entire RDD to a single machine, and you’ll get to see the above result.\n",
    "\n",
    "__Tip:__ be careful when using `collect()`! Running this line of code can possibly cause the driver to run out of memory. That’s why the following approach with `the take()` method is a safer approach if you want to just print a few elements of the RDD. In general, it’s a good principle to limit your result set whenever possible, just like when you’re using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n",
       " '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use the __`map()`__ function again and another lambda function in which you’ll map each entry to a field in a Row.\n",
    "\n",
    "With this SchemaRDD in place, you can easily convert the RDD to a DataFrame with the __`toDF()`__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your DataFrame df, you can inspect it with the methods that you have also used before, namely `first()` and `take()`, but also with `head()` and `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "|         3|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         2|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         4|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         5|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         5|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         5|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         5|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         5|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         5|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         7|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         7|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         6|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "|         7|               2|       1|        -|               0|           0|         2|            .|         2|\n",
      "+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 20 rows \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['households',\n",
       " 'housingMedianAge',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'medianHouseValue',\n",
       " 'medianIncome',\n",
       " 'population',\n",
       " 'totalBedRooms',\n",
       " 'totalRooms']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the columns of your DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: string (nullable = true)\n",
      " |-- housingMedianAge: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- medianHouseValue: string (nullable = true)\n",
      " |-- medianIncome: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- totalBedRooms: string (nullable = true)\n",
      " |-- totalRooms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data types of all `df` columns\n",
    "# df.dtypes\n",
    "\n",
    "# Print the schema of `df`\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the columns are in **String** type, we need to convert them to **Float**. We'll write a function to convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: float (nullable = true)\n",
      " |-- housingMedianAge: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- medianHouseValue: float (nullable = true)\n",
      " |-- medianIncome: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- totalBedRooms: float (nullable = true)\n",
      " |-- totalRooms: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of `df`\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedRooms|\n",
      "+----------+-------------+\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "|       2.0|         null|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('population','totalBedRooms').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|housingMedianAge|count|\n",
      "+----------------+-----+\n",
      "|             2.0| 8207|\n",
      "|             1.0|12433|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+--------+---------+----------------+------------+------------------+-------------+-----------------+\n",
      "|summary|       households|   housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|        population|totalBedRooms|       totalRooms|\n",
      "+-------+-----------------+-------------------+--------+---------+----------------+------------+------------------+-------------+-----------------+\n",
      "|  count|            20640|              20640|   20640|        0|           20640|       20640|             20640|            0|            20640|\n",
      "|   mean| 4.54312015503876| 1.3976259689922481|     1.0|     null|             0.0|         0.0|3.8197674418604652|         null|5.166036821705426|\n",
      "| stddev|2.873140433308782|0.48941920984696774|     0.0|     null|             0.0|         0.0| 2.923293251863948|         null| 3.14998088304445|\n",
      "|    min|              0.0|                1.0|     1.0|     null|             0.0|         0.0|               0.0|         null|              0.0|\n",
      "|    max|              9.0|                2.0|     1.0|     null|             0.0|         0.0|               9.0|         null|              9.0|\n",
      "+-------+-----------------+-------------------+--------+---------+----------------+------------+------------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing\n",
    "### Preprocessing The Target Values\n",
    "First, let’s start with the `medianHouseValue`, your dependent variable. To facilitate your working with the target values, you will express the house values in units of `100,000`. That means that a target such as `452600.000000` should become `4.526`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(households=3.0, housingMedianAge=2.0, latitude=1.0, longitude=None, medianHouseValue=0.0, medianIncome=0.0, population=2.0, totalBedRooms=None, totalRooms=2.0),\n",
       " Row(households=2.0, housingMedianAge=2.0, latitude=1.0, longitude=None, medianHouseValue=0.0, medianIncome=0.0, population=2.0, totalBedRooms=None, totalRooms=2.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DISCLAIMERS: THIS IS NOT NECESSARY SINCE THE MEDIANHOUSEVALUE IS ALREADY PREPROCESSED\n",
    "\n",
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "You’re going to add the following columns to the data set:\n",
    "\n",
    "* `Rooms per household` which refers to the number of rooms in households per block group;\n",
    "* `Population per household` which basically gives you an indication of how many people live in households per block group; And\n",
    "* `Bedrooms per room` which will give you an idea about how many rooms are bedrooms per block group;\n",
    "\n",
    "As you’re working with DataFrames, you can best use the `select()` method to select the columns that you’re going to be working with, namely `totalRooms`, `households`, and `population`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=3.0, housingMedianAge=2.0, latitude=1.0, longitude=None, medianHouseValue=0.0, medianIncome=0.0, population=2.0, totalBedRooms=None, totalRooms=2.0, roomsPerHousehold=0.6666666666666666, populationPerHousehold=0.6666666666666666, bedroomsPerRoom=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-order and select columns for the analysis\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "Now that you have re-ordered the data, you’re ready to normalize the data. Or almost, at least. There is just one more step that you need to go through: separating the features from the target variable. In essence, this boils down to isolating the first column in your DataFrame from the rest of the columns.\n",
    "\n",
    "In this case, you’ll use the `map()` function that you use with RDDs to perform this action. You also see that you make use of the `DenseVector()` function. A dense vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.\n",
    "\n",
    "Next, you go back to making a DataFrame out of the `input_data` and you re-label the columns by passing a list as a second argument. This list consists of the column names `\"label\"` and `\"features\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can finally scale the data. You can use Spark ML to do this: this library will make machine learning on big data scalable and easy. You’ll find tools such as ML algorithms and everything you need to build practical ML pipelines. In this case, you don’t need to do that much preprocessing so a pipeline would maybe be overkill, but if you want to look into it, definitely consider visiting the [this page](https://spark.apache.org/docs/latest/ml-pipeline.html)\n",
    "\n",
    "The input columns are the features, and the output column with the rescaled that will be included in the `scaled_df` will be named `\"features_scaled\"`\n",
    "\n",
    "Let’s take a look at your DataFrame and the result. You see that, indeed, a third column `features_scaled` was added to your DataFrame, which you can use to compare with features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=DenseVector([nan, 2.0, 3.0, 0.0, 0.6667, 0.6667, nan]), features_scaled=DenseVector([nan, 0.6842, 1.0442, 0.0, nan, nan, nan])),\n",
       " Row(label=0.0, features=DenseVector([nan, 2.0, 2.0, 0.0, 1.0, 1.0, nan]), features_scaled=DenseVector([nan, 0.6842, 0.6961, 0.0, nan, nan, nan]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Machine Learning Model With Spark ML\n",
    "With all the preprocessing done, it’s finally time to start building your Linear Regression model! Just like always, you first need to split the data into training and test sets. Luckily, this is no issue with the `randomSplit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ that the argument `elasticNetParam` corresponds to `α` or the vertical intercept and that the `regParam` or the regularization paramater corresponds to `λ`. Go [here](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your model in place, you can generate predictions for your test data: use the `transform()` method to predict the labels for your `test_data`. Then, you can use RDD operations to extract the predictions as well as the true labels from the DataFrame and zip these two values together in a list called `predictionAndLabel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(nan, 0.0), (nan, 0.0), (nan, 0.0), (nan, 0.0), (nan, 0.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "Looking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good your model actually is. You can first start by printing out the coefficients and the intercept of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients\n",
    "\n",
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError\n",
    "\n",
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Go…\n",
    "Before you go, make sure to stop the SparkSession with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
